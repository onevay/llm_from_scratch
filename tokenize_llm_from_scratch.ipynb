{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMir9re4sSSC+DjTffthIlz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onevay/llm_from_scratch/blob/main/tokenize_llm_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text tokenization"
      ],
      "metadata": {
        "id": "SiujC0EErpvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg3SpfZrbCkQ",
        "outputId": "d5263ec7-f759-4738-dbea-7696128600e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 420297\n",
            "Start data: \f\fBuild a Large Language Model (From Scratch)\n",
            " 1.   welcome\n",
            " 2.   1_Understanding_Large_Language_Mod\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/Build-a-Large-Language-Model-_From-Scratch_-.txt\", \"r\") as f:\n",
        "    row_text = f.read()\n",
        "\n",
        "print(f\"Total number of characters: {len(row_text)}\")\n",
        "print(f\"Start data: {row_text[:100]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "example = \"This is example,  rate.\"\n",
        "result = re.split(r\"(\\s)+\", example)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXRj0pareUNU",
        "outputId": "6f2d40f2-26bd-4f94-a727-694a1edcf8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', ' ', 'is', ' ', 'example,', ' ', 'rate.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r\"([,.!?]|\\s)+\", example)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgM0qBaXgQ3P",
        "outputId": "0ab08921-b1e6-4aa3-ee1e-dc5e6340f04b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', ' ', 'is', ' ', 'example', ' ', 'rate', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. This --a test.\"\n",
        "\n",
        "result = re.split(r\"([,.:;{}!?'\\\"|/]|\\s|--)+\", text)\n",
        "result = [token for token in result if token.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-WaJ3pAldjg",
        "outputId": "92ea1d76-e915-47a3-e8b4-e5f1a0e947bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'world', 'This', '--', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens = re.split(r\"([,.:;{}!?'\\\"|/]|\\s|--)+\", row_text)\n",
        "text_tokens = [token for token in text_tokens if token.strip()]\n",
        "print(f\"Text was split on {len(text_tokens)} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO_2qycvrX6w",
        "outputId": "f09c507e-169a-4086-b2e0-300715de30e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text was split on 69444 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token: id for id, token in enumerate(sorted(set(text_tokens)))}\n",
        "for idx, i in enumerate(vocab.items()):\n",
        "    if idx > 30:\n",
        "        break\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhDP5h8PtpIy",
        "outputId": "32bf4a2a-855f-4912-c2e7-9b1dbc5281e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "('#', 2)\n",
            "('###', 3)\n",
            "('#A', 4)\n",
            "('#B', 5)\n",
            "('#C', 6)\n",
            "('#D', 7)\n",
            "('#E', 8)\n",
            "('#F', 9)\n",
            "('#G', 10)\n",
            "('#H', 11)\n",
            "('#I', 12)\n",
            "('#J', 13)\n",
            "('#K', 14)\n",
            "('#L', 15)\n",
            "('#M', 16)\n",
            "('$30', 17)\n",
            "('$30)', 18)\n",
            "('$4', 19)\n",
            "('$690', 20)\n",
            "('%', 21)\n",
            "('%timeit', 22)\n",
            "(\"'\", 23)\n",
            "('(', 24)\n",
            "('((global_step', 25)\n",
            "('()\\\\', 26)\n",
            "('(-inf)', 27)\n",
            "('(-âˆž)', 28)\n",
            "('(0', 29)\n",
            "('(0)', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdfSVMzJ61vF",
        "outputId": "4c22e256-ac02-4517-9bbc-703ee53abb56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer class"
      ],
      "metadata": {
        "id": "-KVgOB5EwYgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "eMBa4ge6uTXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "sample_text = \"\"\"Training deep neural networks with many layers can sometimes prove\n",
        "challenging due to issues like vanishing or exploding gradients.\"\"\"\n",
        "ids = tokenizer.encode(sample_text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw8iU410vW8j",
        "outputId": "d3ffab14-d2dc-4d0b-d1fe-0ab7b7ca22e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2229, 3304, 4930, 4928, 6935, 4669, 4497, 2858, 6100, 5526, 2903, 3523, 6511, 4386, 4554, 6810, 5115, 3771, 4049, 283]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V2"
      ],
      "metadata": {
        "id": "KmiOa8Uu6VEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = re.split(r\"([,.:_;{}()!?'\\\"|/%]|\\s|--)+\", row_text)\n",
        "all_tokens = [token for token in all_tokens if token is not None and token.strip()]\n",
        "all_tokens = sorted(set(all_tokens))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token: id for id, token in enumerate(all_tokens)}\n",
        "print(f\"Vocab lenght: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QE7Idrq6NUF",
        "outputId": "36179d08-ff18-4742-980b-86a88f7ff8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab lenght: 5879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r\"([,.:_;{}()!?'\\\"|/%]|\\s|--)+\", text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "dYx7yHvu80u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizerv2 = SimpleTokenizerV2(vocab)"
      ],
      "metadata": {
        "id": "A3l3uUpJ9RhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizerv2.encode(\"Hello, simple example\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1H2f2n3-VCR",
        "outputId": "0b5a6efb-69c2-4a37-9955-9c37f3e45f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1434, 5099, 3155]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizerv2.encode(\"Bebebe\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H29cdpH1-khi",
        "outputId": "16907bd0-25b7-46a7-db9d-457b61a70dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5878]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizerv2.decode([5878])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2P0EsK2r-eig",
        "outputId": "c9e2e6f2-d616-4165-bf9a-9f00212d7c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiktoken BPE tokenizer"
      ],
      "metadata": {
        "id": "XZMChALDDyPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "escf4ikVD66q",
        "outputId": "990fccb4-e081-47eb-84cf-d0e2768a9dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(f\"Tiktoken version: {importlib.metadata.version(\"tiktoken\")}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOtj47NUECzu",
        "outputId": "3e353f5c-2782-402b-a366-0b8bd246249d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiktoken version: 0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiktokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "fJby2uS5FGIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"Xenova/gpt-4\")"
      ],
      "metadata": {
        "id": "_CeBfG_TErcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "# tokens1 = tiktokenizer.encode(text)\n",
        "tokens2 = tokenizer.encode(text)\n",
        "# print(f\"tokens1: {tokens1}\")\n",
        "print(f\"tokens2: {tokens2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWYlAyDmFkeK",
        "outputId": "71583142-63cd-480a-eb95-0ea8bc965a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens2: [9906, 11, 656, 499, 1093, 15600, 30, 220, 100257, 763, 279, 7160, 32735, 7317, 2492, 1073, 1063, 16476, 17826, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input-target"
      ],
      "metadata": {
        "id": "_UaMqN5lJ6v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens = tokenizer.encode(row_text)\n",
        "inc_tokens = text_tokens[1000:]"
      ],
      "metadata": {
        "id": "Nz-AxxHuJ-1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05600a98-1493-424c-f710-606596e35e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (99736 > 8192). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 4\n",
        "for i in range(window_size):\n",
        "    input = inc_tokens[:i+1]\n",
        "    target = inc_tokens[i+1]\n",
        "    print(f\"Input--> {input}\")\n",
        "    print(f\"Target-->{'       '*(i+1)}{target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1VVt8mNKPq7",
        "outputId": "6ef8c1df-c614-4945-e8e6-f3bfcdcc8af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input--> [2038]\n",
            "Target-->       323\n",
            "Input--> [2038, 323]\n",
            "Target-->              42129\n",
            "Input--> [2038, 323, 42129]\n",
            "Target-->                     1169\n",
            "Input--> [2038, 323, 42129, 1169]\n",
            "Target-->                            552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 4\n",
        "for i in range(window_size):\n",
        "    input = inc_tokens[:i+1]\n",
        "    target = inc_tokens[i+1]\n",
        "    print(f\"Input--> {tokenizer.decode(input)}\")\n",
        "    print(f\"Target-->{'       '*(i+1)}{tokenizer.decode(target)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qziQwjhLGSW",
        "outputId": "2940661c-58eb-4c4f-ec40-185a6a5afcb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input-->  information\n",
            "Target-->        and\n",
            "Input-->  information and\n",
            "Target-->               subt\n",
            "Input-->  information and subt\n",
            "Target-->                     let\n",
            "Input-->  information and subtlet\n",
            "Target-->                            ies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "MoUCYFbVM8Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "\n",
        "    def __init__(self, text, max_length, tokenizer, stride=1):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        self.tokens = tokenizer.encode(text)\n",
        "        for i in range(0, len( self.tokens) - max_length, stride):\n",
        "            input_row = torch.tensor( self.tokens[i:i+max_length])\n",
        "            target_row = torch.tensor(self.tokens[i+1:i+max_length+1])\n",
        "            self.input_ids.append(input_row)\n",
        "            self.target_ids.append(target_row)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, ids):\n",
        "        return self.input_ids[ids], self.target_ids[ids]"
      ],
      "metadata": {
        "id": "qI-QcIXAM-QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained(\"Xenova/gpt-4\")\n",
        "\n",
        "    dataset = GPTDatasetV1(txt, max_length, tokenizer, stride)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "cH9tNeEQPrvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/Build-a-Large-Language-Model-_From-Scratch_-.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    row_text = f.read()"
      ],
      "metadata": {
        "id": "9TbTRdxtQUo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(row_text, batch_size=4, max_length=4, stride=156, shuffle=False, drop_last=True, num_workers=0)\n",
        "#stride help for fix overfitting\n",
        "iterator = iter(dataloader)\n",
        "first_step = next(iterator)\n",
        "print(f\"First step: {first_step}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTwbVMEQQjKg",
        "outputId": "ca9b235c-9009-4c22-9613-f954f841222e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (99736 > 8192). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First step: [tensor([[  200,   200, 11313,   264],\n",
            "        [  198, 13359,   499,   369],\n",
            "        [11537,   449,   856,   990],\n",
            "        [   11, 23391,   912,   198]]), tensor([[  200, 11313,   264, 20902],\n",
            "        [13359,   499,   369, 23395],\n",
            "        [  449,   856,   990,    11],\n",
            "        [23391,   912,   198, 42641]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "5AT8nkRHdAgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERH5oufFdFUg",
        "outputId": "b5d62ed2-1144-476f-aeb1-f9b7041ae51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as download_api\n",
        "embedding_model = download_api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "T0XySkKUdEKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model.get_vector('Word').shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwbWDcV6f6gl",
        "outputId": "3abed708-4817-4dcb-a242-6061833b7d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([5, 3, 1, 2])\n",
        "\n",
        "vocab_size = 6\n",
        "output_size = 4\n",
        "embedding = torch.nn.Embedding(vocab_size, output_size)"
      ],
      "metadata": {
        "id": "_Xj6BqfGjW1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Embedding weight: {embedding.weight}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OETf21djrzU",
        "outputId": "3c60d9ea-47d0-4f34-d675-8594ffccb256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding weight: Parameter containing:\n",
            "tensor([[ 0.6184,  1.0475, -0.1818,  2.4886],\n",
            "        [ 0.1928, -0.4946, -0.4927, -0.4495],\n",
            "        [ 0.1808, -0.4505,  0.7139, -0.3409],\n",
            "        [ 1.1522,  0.2217,  1.3078, -0.5854],\n",
            "        [-1.4921, -0.5727, -1.4264, -1.0420],\n",
            "        [-1.6150,  1.0893,  1.7864, -0.5186]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Needed ids: {embedding.weight[input_ids]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJkfa3oRj2oO",
        "outputId": "e9b84fb4-317a-4f39-ecad-8e342602a892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Needed ids: tensor([[-1.6150,  1.0893,  1.7864, -0.5186],\n",
            "        [ 1.1522,  0.2217,  1.3078, -0.5854],\n",
            "        [ 0.1928, -0.4946, -0.4927, -0.4495],\n",
            "        [ 0.1808, -0.4505,  0.7139, -0.3409]], grad_fn=<IndexBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = embedding(input_ids)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu6tpgSN5NbH",
        "outputId": "08e5ee73-e12f-4e84-93f2-7cc16669093f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEInwp1IN29J"
      },
      "outputs": [],
      "source": [
        "vocab_size = 81075\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-VPNQ20N29J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463a29f7-61f4-45cd-e2b1-d4036f324857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (99736 > 8192). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    row_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSix55tIN29J",
        "outputId": "4d6b6f3b-7ca5-4515-d0de-cd764492fdae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[  200,   200, 11313,   264],\n",
            "        [20902, 11688,  5008,   320],\n",
            "        [ 3915, 81074,   340,   220],\n",
            "        [   16,    13,   256, 10788],\n",
            "        [  198,   220,    17,    13],\n",
            "        [  256,   220,    16,  6803],\n",
            "        [  910, 10276,  2406,  2812],\n",
            "        [ 2406,  2681, 22357,    82]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AelxeGT-N29K",
        "outputId": "1eac3fd1-4e3b-4945-c625-041a4ca7cd33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRcsE-w8N29K"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-9tgNg-N29K",
        "outputId": "6125b903-820f-4105-d527-f2a93fffd562",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdpXy5SlN29L",
        "outputId": "c91b3d83-4b79-475b-ad05-75acba3d7703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    }
  ]
}